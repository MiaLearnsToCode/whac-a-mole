<!DOCTYPE html>
<html>

<head>
  <title>Whac A Mole</title>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
  <link rel="stylesheet" type="text/css" href="style.css">
</head>

<body>
  <video id="video" playsinline style=" 
      -moz-transform: scaleX(-1);
      -o-transform: scaleX(-1);
      -webkit-transform: scaleX(-1);
      transform: scaleX(-1);
      visibility: hidden;
      ">
  </video>
  <div style="display:none;">
    <img id="left" src="https://www.pngrepo.com/download/286636/hammer.png" >
    <img id="right" src="https://www.pngrepo.com/download/286636/hammer.png">
  </div>
  <canvas id="output" style="position: absolute; top: 0; left: 0;"></canvas>

  <script>
    async function bindPage() {
      // We create an object with the parameters that we want for the model. 
      const poseNetState = {
        algorithm: 'single-pose',
        input: {
          architecture: 'MobileNetV1',
          outputStride: 16,
          inputResolution: 513,
          multiplier: 0.75,
          quantBytes: 2
        },
        singlePoseDetection: {
          minPoseConfidence: 0.1,
          minPartConfidence: 0.5,
        },
        output: {
          showVideo: true,
          showPoints: true,
        },
      }

      // Wair for the posenet model to load
      let poseNetModel = await posenet.load({
        architecture: poseNetState.input.architecture,
        outputStride: poseNetState.input.outputStride,
        inputResolution: poseNetState.input.inputResolution,
        multiplier: poseNetState.input.multiplier,
        quantBytes: poseNetState.input.quantBytes
      })

      // Initiate the video stream

      let video
      let videoWidth = 1728 
      let videoHeight = 1026

      try {
        video = await setupCamera()
        video.play()
      } catch (e) {
        throw e
      }

      async function setupCamera() {
        const video = document.getElementById('video')
        video.width = videoWidth
        video.height = videoHeight

        const stream = await navigator.mediaDevices.getUserMedia({
          'audio': false,
          'video': {
            width: videoWidth,
            height: videoHeight,
          },
        })
        video.srcObject = stream

        return new Promise((resolve) => {
          video.onloadedmetadata = () => resolve(video)
        })
      }

      // functions to draw the hammer over the hand on the canvas. 

      function drawPoint(ctx, y, x, r, image) {
        ctx.beginPath()
        ctx.drawImage(image, x, y, 150, 150)
      }

      function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {
        let rightWrist = keypoints.find(point => point.part === 'rightWrist')
        let imageRight = document.querySelector('#right')

        if (rightWrist.score > minConfidence) {
          const { y, x } = rightWrist.position
          drawPoint(ctx, y * scale, x * scale, 10, imageRight)
        }
      }

      // Once the video stream is ready, we start detecting poses:

      function detectPoseInRealTime(video) {
        const canvas = document.getElementById('output')
        const ctx = canvas.getContext('2d')
        const flipPoseHorizontal = true

        canvas.width = videoWidth
        canvas.height = videoHeight

        async function poseDetectionFrame() {
          let poses = []
          let minPoseConfidence
          let minPartConfidence

          switch (poseNetState.algorithm) {
            case 'single-pose':
              const pose = await poseNetModel.estimatePoses(video, {
                flipHorizontal: flipPoseHorizontal,
                decodingMethod: 'single-person'
              })
              poses = poses.concat(pose);
              minPoseConfidence = +poseNetState.singlePoseDetection.minPoseConfidence
              minPartConfidence = +poseNetState.singlePoseDetection.minPartConfidence
              break
          }

          ctx.clearRect(0, 0, videoWidth, videoHeight)

          if (poseNetState.output.showVideo) {
            ctx.save()
            ctx.scale(-1, 1)
            ctx.translate(-videoWidth, 0)
            ctx.restore()
          }

          // calling the drawKeyPoints function for each pose

          poses.forEach(({ score, keypoints }) => {
            if (score >= minPoseConfidence) {
              if (poseNetState.output.showPoints) {
                drawKeypoints(keypoints, minPartConfidence, ctx)
              }
            }
          });
          requestAnimationFrame(poseDetectionFrame)
        }

        poseDetectionFrame()
      }

      detectPoseInRealTime(video)
    }

    bindPage()
  
  </script>
  

</body>

</html>